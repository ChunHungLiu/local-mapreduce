{"name":"Local-mapreduce","tagline":"","body":"# Local MapReduce\r\n\r\nSimulating Hadoop MapReduce Streaming in a multicore computer. Any mapper, reducer compatible with Hadoop Streaming can work on this shell script.\r\n\r\n## Performance Comparison\r\n\r\n- single process\r\n\r\n  Elapsed time: 7 mins 9 secs\r\n\r\n  ```console\r\n  $ time pv citeseerx_descriptions_sents.txt.300000 | python map.py | sort -k1 | python reduce.py > result_single\r\n  37.6MiB 0:01:12 [ 528KiB/s] [====================================================================================================>] 100%\r\n  pv citeseerx_descriptions_sents.txt.300000  0.06s user 0.18s system 0% cpu 1:12.81 total\r\n  python map.py  156.55s user 2.34s system 76% cpu 3:27.56 total\r\n  sort -k1  67.68s user 1.94s system 16% cpu 7:09.31 total\r\n  python reduce.py > result_single  217.10s user 0.26s system 50% cpu 7:09.38 total\r\n  ```\r\n\r\n- localmapreduce\r\n\r\n  Elapsed time: 1 min 18 secs. Five times faster\r\n\r\n\r\n  ```console\r\n  $ rm -r result ; time pv citeseerx_descriptions_sents.txt.300000 | ./localmapreduce 2m 16 'python map.py' 'python reduce.py' result\r\n  37.6MiB 0:00:23 [1.58MiB/s] [====================================================================================================>] 100%\r\n  pv citeseerx_descriptions_sents.txt.300000  0.01s user 0.03s system 0% cpu 23.789 total\r\n  ./localmapreduce 2m 16 'python map.py' 'python reduce.py' result  1024.95s user 11.64s system 1318% cpu 1:18.63 total\r\n  ```\r\n\r\n## Prerequisite\r\n\r\n- Essential\r\n    - bash\r\n    - GNU parallel\r\n    - sort\r\n    - python\r\n\r\n- Recommanded\r\n    - pv\r\n\r\n## Usage\r\n\r\n```console\r\n$ cat <data> | ./localmapreduce <chunk size> <num of reducer> <mapper> <reducer> <output directory>\r\n$ ./localmapreduce <chunk size> <num of jobs> <mapper> <reducer> <output directory> < <data>\r\n```\r\n\r\n- `<chunk size>`: Split input data into chunks with `<chunk size>`. The number of chunks equals to the total number of mapper.\r\n- `<num of reducer>`: Each output line from mappers would then be hashed into `<num of reducer>` different reducer.\r\n- `<mapper>`, `<reducer>`: Any executable shell command can be mapper or reducer.\r\n- `<output directory>`: The output directory.\r\n\r\n\r\n## Mapper Output Format\r\n\r\nMapper reads input from Standard Input and prints `key<TAB>value` per line to Standard Output.\r\n\r\n\r\nExample:\r\n\r\n```\r\nhello world\t\t3,15 4,12 16,33\r\nworld peace\t\t2,11 7,44 9,59\r\njohn doe\t\t5,21 8,5 11,37\r\nworld peace\t\t2,4 3,60 6,28\r\njohn doe\t\t9,89 5,39 11,2\r\njohn doe\t\t1,56 3,62 8,42\r\n```\r\n\r\n## Reducer Input Format\r\n\r\n\r\nAs in Hadoop Streaming, all lines with the same key will be grouped together and pass to the same reducer.\r\n\r\nExample:\r\n\r\n```\r\nhello world\t\t3,15 4,12 16,33\r\njohn doe\t\t5,21 8,5 11,37\r\njohn doe\t\t9,89 5,39 11,2\r\njohn doe\t\t1,56 3,62 8,42\r\nworld peace\t\t2,11 7,44 9,59\r\nworld peace\t\t2,4 3,60 6,28\r\n```\r\n\r\n## Mapper Reducer Example 1: Word Count\r\n\r\nMapper: `tr -sc \"a-zA-Z\" \"\\n\"`\r\nMapper testing:\r\n\r\n```console\r\n$ echo 'aaa bbb ccc\\naaa bbb aaa' | tr -sc \"a-zA-Z\" \"\\n\"\r\naaa\r\nbbb\r\nccc\r\naaa\r\nbbb\r\naaa\r\n```\r\n\r\nReducer: `uniq -c`\r\nReducer testing:\r\n\r\n```console\r\n$ echo 'aaa bbb ccc\\naaa bbb aaa' | tr -sc \"a-zA-Z\" \"\\n\" | sort -k 1,1 -t $'\\t' | uniq -c\r\n      3 aaa\r\n      2 bbb\r\n      1 ccc\r\n```\r\n\r\nRun mapper and reducer with local mapreduce\r\n\r\n```console\r\n$ cat data | ./localmapreduce 5m 8  'tr -sc \"a-zA-Z\" \"\\n\"' 'uniq -c' out\r\n$ cat out/* | head\r\n 148601 a\r\n  10605 A\r\n     19 aa\r\n      1 Aa\r\n      9 AA\r\n      1 aaai\r\n      5 AAAI\r\n      4 Aachen\r\n      2 AAIA\r\n     23 AAM\r\n```\r\n\r\n\r\n\r\n## mapper reducer Example 2ï¼šngram count in python\r\n\r\n### mapper\r\n\r\nmapper *nc-map.py*\r\n\r\n```python\r\n#!/usr/bin/env python\r\nimport re\r\ndef tokens(str1): return re.findall('[a-z]+', str1.lower())\r\n\r\ndef to_ngrams( words, length):\r\n    return zip(*[words[i:] for i in range(length)])  \r\n\r\nimport fileinput\r\nfrom collections import Counter\r\nfor line in fileinput.input():\r\n    words = tokens(line)\r\n    for n in range(1, 6):\r\n        ngrams_counts = Counter(to_ngrams(words, n))\r\n        for ngram, count in ngrams_counts.iteritems():\r\n            print  '{}\\t{}'.format(' '.join(ngram), count)\r\n```\r\n\t\r\nmapper testing\r\n\r\n```console\r\n$ echo 'aaa bbb ccc\\naaa bbb aaa' | python nc-map.py\r\nbbb     1\r\nccc     1\r\naaa     1\r\naaa bbb 1\r\nbbb ccc 1\r\naaa bbb ccc     1\r\nbbb     1\r\naaa     2\r\naaa bbb 1\r\nbbb aaa 1\r\naaa bbb aaa     1\r\n```\r\n\r\n\r\n\r\n### reducer\r\n\r\nreducer *nc-reduce.py*\r\n\r\n```python\r\n#!/usr/bin/env python\r\nimport fileinput\r\nfrom collections import Counter, defaultdict\r\nngram_counts = defaultdict(Counter)\r\nfor line in fileinput.input():\r\n    ngram, count = line.split('\\t', 1)\r\n    ngram, count = tuple(ngram.split(' ')), int(count)\r\n    length = len(ngram)\r\n    ngram_counts[length][ngram] += count\r\n\r\nfor length in ngram_counts:\r\n    for ngram, count in ngram_counts[length].iteritems():\r\n        print '{}\\t{}'.format(' '.join(ngram), count)\r\n```\r\n\r\nreducer testing\r\n\r\n```console\r\n$ echo 'aaa bbb ccc\\naaa bbb aaa' | python nc-map.py | sort -k 1,1 -t $'\\t' | python nc-reduce.py\r\nbbb     2\r\nccc     1\r\naaa     3\r\naaa bbb 2\r\nbbb ccc 1\r\nbbb aaa 1\r\naaa bbb ccc     1\r\naaa bbb aaa     1\r\n```\r\n\r\n### run with localmapreduce  ###\r\n\r\n```console\r\n$ cat data | ./localmapreduce 5m 16  'python nc-map.py' 'python nc-reduce.py' nc-out\r\n$ cat nc-out/* | head\r\nscoring 121\r\nkazhdan 2\r\ncrex    3\r\nblastsets       3\r\nglycoside       1\r\nbrieflyreview   1\r\nseriesparallel  2\r\nstle    1\r\nabar    1\r\nparalleled      4\r\n```\r\n\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}